{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Iowa House Prices\n",
    "This Kaggle project predicted house prices in Ames, Iowa with 79 original features (2006-2010).\n",
    "The training set had 1460 observations and the test set had 1459 observations. I tried linear regression, Elastic Net, Random Forest and XGBoost, and finally built a weighted average meta model. My predictions ranked top 10% according to Mean Squared Error in November, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "#import libraries--feature engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#import libraries--modeling\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore warnings from sklearn and seaborn\n",
    "\n",
    "#setup graphs\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix outlier: YrSold is earlier than YrBuilt for the observation 1089 in the test data\n",
    "test.loc[1089][\"YrSold\"] = 2009\n",
    "test.loc[1089][\"YrActualAge\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the 'Id' column then drop it from original datasets--not used in modeling\n",
    "#axis = 1 indicates col\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Outliers\n",
    "#use a scatter plot to observation the relationshiop between living areas and prices\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "#Delete outliers in the bottom-right corner of the scatter plot\n",
    "train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
    "\n",
    "#Check distribution again\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train['GrLivArea'], train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target variable\n",
    "#Check the distribution of target variable: saleprice\n",
    "sns.distplot(train['SalePrice'] , fit=norm);\n",
    "\n",
    "#Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Plot the distribution of salesprice\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "# Label is right-skewed. necessary to perform log transformation to make it more normally distributed.\n",
    "#use the numpy fuction log1p to apply log(1+x): plus 1 to avoid -inf\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#Check the new distribution \n",
    "sns.distplot(train['SalePrice'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution again\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate train and test data to clean data together\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.SalePrice.values\n",
    "all = pd.concat((train, test)).reset_index(drop=True)\n",
    "all.drop(['SalePrice'], axis=1, inplace=True)\n",
    "print(\"all size is : {}\".format(all.shape))\n",
    "\n",
    "#Assign \"None\" to missing values accordingly:\n",
    "for col in ('MSSubClass', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "    all[col] = all[col].fillna('None')\n",
    "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood in the training set\n",
    "#median, mean functions are not affected by missing values. first, obtain the median of training data\n",
    "nbh_lot = train.groupby(train.Neighborhood)[['LotFrontage']].median()\n",
    "#med_lot = neigh_lot.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\"median\")\n",
    "#all[\"LotFrontage\"] = all[\"LotFrontage\"].fillna(med_lot)\n",
    "#all.loc[all.Neighborhood.isin(neigh_lot.Neighborhood), ['LotFrontage']] = neigh_lot['LotFrontage']\n",
    "all = all.merge(nbh_lot, on=[\"Neighborhood\"], how='left', suffixes=('','_'))\n",
    "all['LotFrontage'] = all['LotFrontage'].fillna(all['LotFrontage_']).astype(int)\n",
    "all = all.drop('LotFrontage_', axis=1)\n",
    "\n",
    "#all[\"LotFrontage\"] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "#GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n",
    "#BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement\n",
    "for col in ('MasVnrArea', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    all[col] = all[col].fillna(0)\n",
    "#Remove \"Utilities\"-- For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA .\n",
    "all = all.drop(['Utilities'], axis=1)\n",
    "#Functional : data description says NA means typical\n",
    "all[\"Functional\"] = all[\"Functional\"].fillna(\"Typ\")\n",
    "#vars with only one NA value, use mode of this var in the training set to prevent data leakage\n",
    "for col in ('KitchenQual', 'Electrical', 'Exterior1st', 'Exterior2nd', 'MSZoning', 'SaleType'):\n",
    "    all[col] = all[col].fillna(train[col].mode()[0])\n",
    "\n",
    "#check if there is any remaining missing values\n",
    "all[all.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing OverallCond into a categorical variable\n",
    "all['OverallCond'] = all['OverallCond'].apply(str)\n",
    "\n",
    "#Year and month sold are transformed into categorical features.\n",
    "all['YrSold'] = all['YrSold'].astype(str)\n",
    "all['MoSold'] = all['MoSold'].astype(str)\n",
    "\n",
    "#LabelEncode categorical variables with orders: different numbers in the same col\n",
    "cates = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in cates:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all[c].values)) \n",
    "    all[c] = lbl.transform(list(all[c].values))\n",
    "\n",
    "#add a feature: total areas\n",
    "all['TotalSF'] = all['TotalBsmtSF'] + all['1stFlrSF'] + all['2ndFlrSF']\n",
    "\n",
    "#use one hot code transfer categorical values -- preparing for PCA\n",
    "all = pd.get_dummies(all)\n",
    "print(all.shape)\n",
    "\n",
    "\n",
    "#clean Skewed features\n",
    "num_vars = all.dtypes[all.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_vars = all[num_vars].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkewness of numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_vars})\n",
    "skewness.head(15)\n",
    "\n",
    "# apply Box Cox Transformation to (highly) skewed features\n",
    "skewness = skewness[abs(skewness) > 0.8]\n",
    "skewed_features = skewness.index\n",
    "lam = 0.25\n",
    "for f in skewed_features:\n",
    "    all[f] = boxcox1p(all[f], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test data again\n",
    "train = all[:ntrain]\n",
    "test = all[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize dataset -- preparing for PCA: too many features\n",
    "scaler = StandardScaler()\n",
    "#fit on training set only.\n",
    "scaler.fit(train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "#pca: reduction dimensionality of features\n",
    "#make an instance of the Model: scikit-learn choose the minimum number of principal components such that 95% of the variance is retained\n",
    "pca = PCA(.95)\n",
    "pca.fit(train)\n",
    "train = pca.transform(train)\n",
    "test = pca.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a simple linear regression first\n",
    "metric = 'neg_mean_squared_error'\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "print(f\"{np.sqrt(-cross_val_score(LinearRegression(), train, y_train, cv=kfold, scoring=metric)).mean():.4f} Error\")\n",
    "\n",
    "#Grid search of elastic net, random forest and xgb\n",
    "#LightGBM takes forever to run\n",
    "#KernelRidge generates errors: Mean Squared Logarithmic Error cannot be used when targets contain negative values\n",
    "#ElasticNet\n",
    "elastic = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0004, l1_ratio=1, random_state=1))\n",
    "param_grid = {\n",
    "    'elasticnet__alpha' : np.linspace(0.0001, 0.001, 10),\n",
    "    'elasticnet__l1_ratio' : np.linspace(0.2, 2, 20),\n",
    "}\n",
    "search = GridSearchCV(elastic, param_grid, cv=10, scoring=metric, n_jobs=-1)\n",
    "search.fit(train, y_train)\n",
    "best_params_ela = search.best_params_\n",
    "print(f\"{search.best_params_}\")\n",
    "print(f\"{np.sqrt(-search.best_score_):.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "rdf = make_pipeline(RobustScaler(), RandomForestRegressor(max_depth=4, n_estimators=100, random_state=1))\n",
    "param_grid={\n",
    "            'max_depth': range(2,4),\n",
    "            'n_estimators': (50, 100),\n",
    "        }\n",
    "search = GridSearchCV(RandomForestRegressor(), param_grid, cv=10, scoring=metric, n_jobs=-1)   \n",
    "search.fit(train, y_train)\n",
    "best_params_forest = search.best_params_\n",
    "print(f\"{search.best_params_}\")\n",
    "print(f\"{np.sqrt(-search.best_score_):.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBboost\n",
    "xgbreg = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                             colsample_bytree=0.45, gamma=0.046, \n",
    "                             learning_rate=0.03, max_depth=2, \n",
    "                             min_child_weight=0.4, n_estimators=10,\n",
    "                             reg_alpha=0.47, reg_lambda=0.8,\n",
    "                             subsample=0.5, random_state=1, n_jobs=-1)\n",
    "\n",
    "param_grid = {\n",
    "    'xgb__max_depth' : [2, 3],\n",
    "    'xgb__estimators' : [10, 25, 50],\n",
    "    \"xgb__learning_rate\" : [0.01, 0.02],\n",
    "    \"xgb__min_child_weight\" : [0.2, 0.3],\n",
    "    }\n",
    "search = GridSearchCV(xgbreg, param_grid, cv=3, scoring=metric, n_jobs=-1)\n",
    "search.fit(train, y_train)\n",
    "best_params_xgb = search.best_params_\n",
    "print(f\"{search.best_params_}\")\n",
    "print(f\"{np.sqrt(-search.best_score_):.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elastic net\n",
    "elastic_net = make_pipeline(RobustScaler(), ElasticNet(alpha=best_params_ela['elasticnet__alpha'], \n",
    "                                                   l1_ratio=best_params_ela['elasticnet__l1_ratio'], random_state=1))\n",
    "forest = make_pipeline(RobustScaler(), RandomForestRegressor(max_depth=best_params_forest[\"max_depth\"], n_estimators=best_params_forest[\"n_estimators\"],  random_state=1))\n",
    "xgb_reg = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                             colsample_bytree=0.45, gamma=0.046, \n",
    "                             max_depth=best_params_xgb[\"xgb__max_depth\"], n_estimators=best_params_xgb[\"xgb__estimators\"], \n",
    "                             learning_rate = best_params_xgb['xgb__learning_rate'], min_child_weight= best_params_xgb[\"xgb__min_child_weight\"],\n",
    "                             reg_alpha=0.47, reg_lambda=0.8,\n",
    "                             subsample=0.5, random_state=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a meta model\n",
    "classifiers = [elastic_net, forest, xgb_reg]\n",
    "clf_names   = [\"elastic_net\", \"random_forest\", \"XGBboost\"]\n",
    "weights = [0.8, 0.15, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "predictions_exp= []\n",
    "for clf_name, clf, weight in zip(clf_names, classifiers, weights):\n",
    "    print(f\"{clf_name} {np.sqrt(-cross_val_score(clf, train, y_train, scoring=metric).mean()):.5f}\")\n",
    "    clf.fit(train, y_train)\n",
    "    preds = clf.predict(test)\n",
    "    predictions_exp.append(weight*np.expm1(preds))\n",
    "prediction_final = pd.DataFrame(predictions_exp).sum().T.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
